{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Lennard Alms // Compare to boilerplate to see what parts are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7QhqkVHHXe9m",
    "outputId": "31f3f734-8ec0-4675-8227-acba38058cc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 12:14:23 URL:https://storage.googleapis.com/laubenthal_spatiolab/spatio_merged_data_iss.zip [37282886/37282886] -> \"spatio_merged_data_iss.zip\" [1]\n",
      "Archive:  spatio_merged_data_iss.zip\n",
      "   creating: input/\n",
      "  inflating: input/.DS_Store         \n",
      "  inflating: __MACOSX/input/._.DS_Store  \n",
      "  inflating: input/grid_ML.geojson   \n",
      "  inflating: __MACOSX/input/._grid_ML.geojson  \n",
      "  inflating: input/internet_ML.csv   \n",
      "  inflating: __MACOSX/input/._internet_ML.csv  \n",
      "  inflating: input/satelite.png      \n",
      "  inflating: __MACOSX/input/._satelite.png  \n",
      "  inflating: input/weather.csv       \n",
      "  inflating: __MACOSX/input/._weather.csv  \n",
      "  inflating: input/social_pulse_ML.csv  \n",
      "  inflating: __MACOSX/input/._social_pulse_ML.csv  \n"
     ]
    }
   ],
   "source": [
    "# edit this when working in a local environment\n",
    "!wget \"https://storage.googleapis.com/laubenthal_spatiolab/spatio_merged_data_iss.zip\" --no-verbose\n",
    "!unzip spatio_merged_data_iss.zip\n",
    "!rm spatio_merged_data_iss.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZ1nZzGkr5Jg",
    "outputId": "82d8f467-78ae-4a48-f814-31f94b6e6370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'functions'...\n",
      "remote: Enumerating objects: 314, done.\u001b[K\n",
      "remote: Total 314 (delta 0), reused 0 (delta 0), pack-reused 314\u001b[K\n",
      "Receiving objects: 100% (314/314), 170.61 KiB | 425.00 KiB/s, done.\n",
      "Resolving deltas: 100% (197/197), done.\n"
     ]
    }
   ],
   "source": [
    "# edit this when working in a local environment\n",
    "!rm -rf functions\n",
    "!git clone https://github.com/markuslaubenthal/lab_st.git functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPc1Iy8WXivI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, activations\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0WgyYa4Go-6"
   },
   "outputs": [],
   "source": [
    "from functions.preprocessing.DataImport import load_and_scale_internet, load_and_scale_satelite, load_and_scale_social, load_and_scale_weather, create_space_invariant\n",
    "from functions.preprocessing.DataGeneration import generate_dataset, generate_label, getFileHandler, get_datasets_from_file\n",
    "from functions.postprocessing.ErrorEvaluation import calculate_errors\n",
    "from functions.preprocessing.TestTrainSplit import seven_days_train_test_split\n",
    "from functions.model.HadamardLayer import HadamardLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYvtwt9ReLDR"
   },
   "outputs": [],
   "source": [
    "f = getFileHandler(\"training_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJOdgb9tJS-z"
   },
   "outputs": [],
   "source": [
    "internet, internet_origin, internet_min, internet_max = load_and_scale_internet('input/internet_ML.csv', f, log10=True)\n",
    "#satelite = load_and_scale_satelite('input/satelite.png', f)\n",
    "#social = load_and_scale_social('input/social_pulse_ML.csv', f)\n",
    "#weather = load_and_scale_weather('input/weather.csv', f)\n",
    "#hour, weekday, holiday = create_space_invariant(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHXT0DBtM19h"
   },
   "outputs": [],
   "source": [
    "steps_back = np.array([0, 1, 2, 3, 20, 21, 22, 23, 24, 143, 165, 166, 167]) + 1\n",
    "steps_back = steps_back[::-1]\n",
    "x = generate_dataset(internet, steps_back, 168, f, \"x\")\n",
    "y = generate_label(internet, 168, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzWh4D_2x4s3"
   },
   "outputs": [],
   "source": [
    "x_train, x_test = seven_days_train_test_split(x, 168)\n",
    "y_train, y_test = seven_days_train_test_split(y, 168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ah__2l7y4B0Y"
   },
   "outputs": [],
   "source": [
    "y = y[:].reshape(y.shape[0], 100, 100)\n",
    "y_train = y_train[:].reshape(y_train.shape[0], 100, 100)\n",
    "y_test = y_test[:].reshape(y_test.shape[0], 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLj5PcUn4B15"
   },
   "outputs": [],
   "source": [
    "xt = np.zeros((x.shape[0],x.shape[3],x.shape[2],x.shape[1]))\n",
    "yt = np.zeros((x.shape[0],x.shape[2],x.shape[1]))\n",
    "for t in range(x.shape[0]):\n",
    "  xt[t] = x[t].T\n",
    "  yt[t] = y[t].T\n",
    "\n",
    "xt_train = np.zeros((x_train.shape[0],x_train.shape[3],x_train.shape[2],x_train.shape[1]))\n",
    "yt_train = np.zeros((x_train.shape[0],x_train.shape[2],x_train.shape[1]))\n",
    "for t in range(x_train.shape[0]):\n",
    "  xt_train[t] = x_train[t].T\n",
    "  yt_train[t] = y_train[t].T\n",
    "\n",
    "xt_test = np.zeros((x_test.shape[0],x_test.shape[3],x_test.shape[2],x_test.shape[1]))\n",
    "yt_test = np.zeros((x_test.shape[0],x_test.shape[2],x_test.shape[1]))\n",
    "for t in range(x_test.shape[0]):\n",
    "  xt_test[t] = x_test[t].T\n",
    "  yt_test[t] = y_test[t].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7nksQsl4Gfu"
   },
   "outputs": [],
   "source": [
    "xt = xt[:,:,:,:,np.newaxis]\n",
    "xt_train = xt_train[:,:,:,:,np.newaxis]\n",
    "xt_test = xt_test[:,:,:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2j6IrVk4NiF",
    "outputId": "88db23e5-4139-4726-a6bb-1d288148df8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log:True, layers:8, filters:4, patch:(3, 3)\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 82s 2s/step - loss: 0.1508 - val_loss: 0.0717\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0365 - val_loss: 0.0329\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0181 - val_loss: 0.0263\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0153 - val_loss: 0.0130\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0127 - val_loss: 0.0212\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0116 - val_loss: 0.0106\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0076 - val_loss: 0.0058\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0061 - val_loss: 0.0051\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0056 - val_loss: 0.0058\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0054 - val_loss: 0.0048\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0051 - val_loss: 0.0043\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0051 - val_loss: 0.0046\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0048 - val_loss: 0.0041\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0049 - val_loss: 0.0039\n",
      "Epoch 16/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 17/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 18/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 19/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 20/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 21/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 22/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0040 - val_loss: 0.0042\n",
      "Epoch 23/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0041 - val_loss: 0.0034\n",
      "Epoch 24/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0039 - val_loss: 0.0048\n",
      "Epoch 25/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 26/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 27/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0040 - val_loss: 0.0053\n",
      "Epoch 28/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 29/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0037 - val_loss: 0.0032\n",
      "Epoch 30/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 31/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0037 - val_loss: 0.0033\n",
      "Epoch 32/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 33/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 34/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 35/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 36/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 37/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0036 - val_loss: 0.0032\n",
      "Epoch 38/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 39/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0036 - val_loss: 0.0032\n",
      "Epoch 40/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 41/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0035 - val_loss: 0.0035\n",
      "Epoch 42/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 43/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 44/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 45/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0035 - val_loss: 0.0037\n",
      "Epoch 46/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0036 - val_loss: 0.0031\n",
      "Epoch 47/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 48/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 49/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 50/50\n",
      "36/36 [==============================] - 65s 2s/step - loss: 0.0034 - val_loss: 0.0036\n",
      "all:  100.8940390650233\n",
      "test:  104.93188918139049\n",
      "val:  66.93784527729132\n"
     ]
    }
   ],
   "source": [
    "lay = [8]\n",
    "filt = [4]\n",
    "pat = [(3,3)]\n",
    "for _l in lay:\n",
    "  for _f in filt:\n",
    "    for _p in pat:\n",
    "      \n",
    "      name = f'log:True, layers:{_l}, filters:{_f}, patch:{_p}'\n",
    "      print(name)\n",
    "      \n",
    "      model = keras.models.Sequential()\n",
    "      \n",
    "      model.add(keras.Input(shape=(xt.shape[1],xt.shape[2],xt.shape[3])))\n",
    "\n",
    "      model.add(layers.Reshape(((xt.shape[1],xt.shape[2],xt.shape[3],1))))\n",
    "\n",
    "      for __l in range(_l):\n",
    "        ret_seq = __l != _l - 1\n",
    "        model.add(layers.ConvLSTM2D(_f, \n",
    "                                    _p, \n",
    "                                    padding='same', \n",
    "                                    return_sequences=ret_seq, \n",
    "                                    data_format='channels_last'))\n",
    "      \n",
    "      model.add(HadamardLayer(name='H1'))\n",
    "      model.add(layers.Lambda(lambda x : K.sum(x, axis=3)))\n",
    "\n",
    "      lr = 0.01\n",
    "      epochs = 50\n",
    "      model.compile(optimizer=keras.optimizers.Adam(lr=lr, decay= lr/float(epochs)), loss='mse')\n",
    "      model.fit(xt_train, yt_train, validation_data=(xt_test,yt_test),epochs=epochs)\n",
    "\n",
    "      predt = model.predict(xt)\n",
    "      pred = np.zeros(predt.shape)\n",
    "      for t in range(predt.shape[0]):\n",
    "        pred[t] = predt[t].T\n",
    "\n",
    "      calculate_errors(pred, internet_origin, internet_min, internet_max, log10=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dij-E_hGVHTm"
   },
   "outputs": [],
   "source": [
    "def test(_y,_x):\n",
    "  predt = model.predict(xt)\n",
    "  predt[:,_y,_x] = 0\n",
    "  pred = np.zeros(predt.shape)\n",
    "  for t in range(predt.shape[0]):\n",
    "    pred[t] = predt[t].T\n",
    "\n",
    "  calculate_errors(pred, internet_origin, internet_min, internet_max, log10=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Baseline_ConvLSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
